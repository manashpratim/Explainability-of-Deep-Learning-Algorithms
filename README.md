# Explainability-of-Deep-Learning-Algorithms
What factors influence the predictions of Deep learning Algorithms?
# Decription

•	Implemented saliency map [1] (hw3_attribution.py), integrated gradients [2] (hw3_attribution.py), influence-directed explanations [3] (hw3_infl.py) and representer points [4] (hw4_part3.py) to determine the impact of each training point on a target instance in a VGG 16 network

•	Compared these methods using visual comparisons as well as quantitative metrics like Average % Drop and Necessity Ordering [5] (hw3_evaluation.py)

# References
[1] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus- Robert Mueller. How to explain individual classification decisions, 2009.

[2] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365, 2017.

[3] Klas Leino, Linyi Li, Shayak Sen, Anupam Datta, and Matt Fredrikson. Inuence-directed explanations for deep convolutional networks. arXiv preprint arXiv:1802.03788, 2018.

[4] Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection for explaining deep neural networks. In Advances in Neural Information Processing Systems, pages 9291–9301, 2018.

[5] Zifan Wang, PiotrPiotr Mardziel, Anupam Datta, and Matt Fredrikson. Interpreting interpretations: Organizing attribution methods by criteria, 2020.

Note: This project is part of my Homeworks. Current CMU students please refrain from going through the codes.


